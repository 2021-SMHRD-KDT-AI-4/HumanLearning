{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85dcca70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from flask import Flask\n",
    "from flask import request,redirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7bd2cfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "C:\\Users\\SMHRD\\Anaconda3\\envs\\Deep_GPU2021\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import sklearn\n",
    "import IPython\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import subprocess\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pydub.silence import detect_nonsilent\n",
    "import librosa\n",
    "import wave\n",
    "import contextlib\n",
    "from konlpy.tag import Kkma\n",
    "import math\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from typing import List\n",
    "from lexrankr import LexRank\n",
    "import cx_Oracle\n",
    "from moviepy.editor import *\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab13f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = pd.read_csv(\"./불용어.csv\" , encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9aa878",
   "metadata": {},
   "outputs": [],
   "source": [
    "bul_list = []\n",
    "for bul in word_list[\"불용어\"] :\n",
    "    bul_list.append(bul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7208f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.kkma = Kkma()\n",
    "#         self.okt = Okt()\n",
    "#         self.stopwords = ['중인' ,'만큼', '마찬가지', '꼬집었', \"연합뉴스\", \"데일리\", \"동아일보\", \"중앙일보\", \"조선일보\", \"기자\"\n",
    "#         ,\"아\", \"휴\", \"아이구\",\"다른\" ,\"얘기\",\"아이쿠\", \"아이고\", \"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\", \"을\", \"를\", \"에\", \"의\", \"가\",]\n",
    "        self.stopwords = bul_list\n",
    "    def url2sentences(self, text):\n",
    "#     article = Article(url, language='ko')\n",
    "#     article.download()\n",
    "#     article.parse()\n",
    "        sentences = self.kkma.sentences(text)\n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        return sentences\n",
    "    def text2sentences(self, text):\n",
    "        sentences = self.kkma.sentences(text)\n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        return sentences\n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence is not '':\n",
    "                nouns.append(' '.join([noun for noun in self.kkma.nouns(str(sentence))\n",
    "                    if noun not in self.stopwords and len(noun) > 1]))\n",
    "        return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c1dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "        \n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return self.graph_sentence\n",
    "    \n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b309fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fef4d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        if text[:5] in ('http:', 'https'):\n",
    "            self.sentences = self.sent_tokenize.url2sentences(text)\n",
    "        else:\n",
    "            self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "            \n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        \n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "            \n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        \n",
    "        return summary\n",
    "    def keywords(self, word_num=10):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "        return keywords\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8eda1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#영상분석 insert\n",
    "def insert_videos(t):\n",
    "    conn = cx_Oracle.connect(\"hr/hr@222.102.104.172:1521/XE\")\n",
    "    cursor=conn.cursor()\n",
    "    sql = \"insert into videos values(videos_seq.nextval,:1,:2,:3,:4,default,' ',:5)\"\n",
    "    cursor.execute(sql,t)\n",
    "    cursor.close()\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66f35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_video_analyses(t):\n",
    "    conn = cx_Oracle.connect(\"hr/hr@222.102.104.172:1521/XE\")\n",
    "    cursor=conn.cursor()\n",
    "    sql = \"insert into video_analyses values(video_analyses_seq.nextval,:1,:2,:3,:4,:5,null)\"\n",
    "    cursor.execute(sql,t)\n",
    "    cursor.close()\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac5ff534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_keywords(t):\n",
    "    conn = cx_Oracle.connect(\"hr/hr@222.102.104.172:1521/XE\")\n",
    "    cursor=conn.cursor()\n",
    "    sql = \"insert into keywords values(keywords_seq.nextval,:1,:2,:3,:4)\"\n",
    "    cursor.execute(sql,t)\n",
    "    cursor.close()\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a21dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_videos_seq() :\n",
    "    conn = cx_Oracle.connect(\"hr/hr@222.102.104.172:1521/XE\")\n",
    "    cursor=conn.cursor()\n",
    "    sql = \"SELECT VIDEOS_SEQ.NEXTVAL FROM DUAL\"\n",
    "    cursor.execute(sql,)\n",
    "    result=cursor.fetchall()\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "    cursor=conn.cursor()\n",
    "    sql = \"ALTER SEQUENCE VIDEOS_SEQ INCREMENT BY -1\"\n",
    "    cursor.execute(sql,)\n",
    "    cursor.close()\n",
    "    \n",
    "    \n",
    "    cursor=conn.cursor()\n",
    "    sql = \"SELECT VIDEOS_SEQ.NEXTVAL FROM DUAL\"\n",
    "    cursor.execute(sql,)   \n",
    "    result=cursor.fetchall()\n",
    "    cursor.close()\n",
    "    \n",
    "    cursor=conn.cursor()\n",
    "    sql = \"ALTER SEQUENCE VIDEOS_SEQ INCREMENT BY 1\"\n",
    "    cursor.execute(sql,)\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94e95810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "class OktTokenizer:\n",
    "    okt: Okt = Okt()\n",
    "\n",
    "    def __call__(self, text: str) -> List[str]:\n",
    "        tokens: List[str] = self.okt.pos(text, norm=True, stem=True, join=True)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2f60c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "kkm=Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12ef9ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad2d = lambda a, i: a[:, 0:i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0], i-a.shape[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1158e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model('my_h5_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11a3d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_wav(data, sample_rate, start, end):\n",
    "    start *= sample_rate\n",
    "    end *= sample_rate\n",
    "    return data[int(start):int(end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e3b7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_duration(file_path) :\n",
    "    with contextlib.closing(wave.open(file_path,'r')) as f:\n",
    "        frames = f.getnframes()\n",
    "        rate = f.getframerate()\n",
    "        duration = frames / float(rate)\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28a5efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_time(file_path,duration) :\n",
    "    y, sr = librosa.load(file_path, sr=16000)\n",
    "\n",
    "    np_padded_ex_mfcc_list = []\n",
    "    i =0\n",
    "    a=int((duration-1)/0.5)\n",
    "    for time in range(a) :\n",
    "        y_split=split_wav(y,sr,i,i+1)\n",
    "        mfcc = librosa.feature.mfcc(y_split, sr=16000, n_mfcc=100, n_fft=400, hop_length=160)\n",
    "        mfcc = sklearn.preprocessing.scale(mfcc, axis=1)\n",
    "        padded_mfcc = pad2d(mfcc, 100)\n",
    "        np_padded_mfcc=np.array(padded_mfcc)\n",
    "        np_padded_ex_mfcc=np.expand_dims(np_padded_mfcc, -1)\n",
    "        np_padded_ex_mfcc_list.append(np_padded_ex_mfcc)\n",
    "        i +=0.5\n",
    "\n",
    "    word_list=np.argmax(model.predict(np.array(np_padded_ex_mfcc_list)),axis = 1)\n",
    "    \n",
    "    start_time = 0\n",
    "    start_i = 0\n",
    "    end_cnt = 0\n",
    "    time_list2 = []\n",
    "    for i in range(len(word_list)-1) :\n",
    "        if word_list[i] == 1 :\n",
    "            end_cnt += 1\n",
    "            if (word_list[i+1] == 0) & (end_cnt >=2) :\n",
    "                if start_i > 50 :\n",
    "                    time_list2.append([start_time,(i+1)*0.5+0.05])\n",
    "                    start_time =(i+1)*0.5\n",
    "                    start_i = 0\n",
    "        else :\n",
    "            end_cnt = 0\n",
    "        start_i +=1\n",
    "    time_list2.append([time_list2[-1][1],math.floor(duration*100)/100])\n",
    "    return time_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9415c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stt_script(file_path,time_list2,duration) :\n",
    "    import speech_recognition as sr\n",
    "    recognizer = sr.Recognizer()\n",
    "    recognizer.energy_threshold = 300\n",
    "\n",
    "    #데이터 경로 바꿔주기\n",
    "    muli_audio = sr.AudioFile(file_path)\n",
    "\n",
    "    #데이터 경로 바꿔주기\n",
    "    audio_seg= AudioSegment.from_wav(file_path)\n",
    "    total_in_ms= len(audio_seg)\n",
    "\n",
    "    Text_list = []\n",
    "    Text_list2 = []\n",
    "    for start,end in tqdm(time_list2) : #tqdm(range (1,int(total_in_ms/1000/30)+1)) :\n",
    "        with muli_audio as source:\n",
    "            s_audio = recognizer.record(source,offset = start,duration=(end-start))\n",
    "        Text_list.append(recognizer.recognize_google(audio_data=s_audio, language=\"ko-KR\",show_all=True))\n",
    "    last_list = []\n",
    "    for i in Text_list :\n",
    "        if i!= [] :\n",
    "            last_list.append(i['alternative'][0]['transcript'])\n",
    "        else :\n",
    "            last_list.append(\"\")\n",
    "    df=pd.DataFrame(time_list2,columns=['start_time','end_time'])\n",
    "    df[\"script\"]=last_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0502a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_script(data) :\n",
    "    sentence_list = []\n",
    "    for script in data[\"script\"]: \n",
    "        sentence_list.append(kkm.sentences(script))\n",
    "    script_list=[]\n",
    "    new_sentence_list = []\n",
    "    for sentences in sentence_list :\n",
    "        script =[]\n",
    "        for st in sentences :\n",
    "            pos=kkm.pos(st)\n",
    "            if (pos[-1][1] =='EFN') | (pos[-1][1] =='EFA'):\n",
    "                script.append(st +\". \")\n",
    "            elif pos[-1][1] =='EFQ':  \n",
    "                script.append(st +\"? \")\n",
    "            elif (pos[-1][1] =='EFI') | (pos[-1][1] =='EFO') :   \n",
    "                script.append(st +\"! \")\n",
    "            else :\n",
    "                script.append(st +\" \")\n",
    "        script_list.append(script)\n",
    "    for i in range(len(script_list)-1) :\n",
    "        if (script_list[i][-1][-2:] != '. ') & (script_list[i][-1][-2:] != '? ') & (script_list[i][-1][-2:] != '! ') :\n",
    "            # 앞으로 밀착\n",
    "            if len(script_list[i+1][0]) <=20 :\n",
    "                data.iloc[i,1] = data.iloc[i,1] + ((data[\"end_time\"]-data[\"start_time\"])/len(data[\"script\"]))[i]*len(script_list[i+1][0])\n",
    "                data.iloc[i+1,0] =data.iloc[i+1,0] + ((data[\"end_time\"]-data[\"start_time\"])/len(data[\"script\"]))[i]*len(script_list[i+1][0])\n",
    "                script_list[i][-1]=script_list[i][-1] + script_list[i+1][0]\n",
    "                script_list[i+1][0]=[] \n",
    "            # 뒤로 밀착\n",
    "            elif len(script_list[i][-1]) <= 20:\n",
    "                data.iloc[i,1] = data.iloc[i,1] - ((data[\"end_time\"]-data[\"start_time\"])/len(data[\"script\"]))[i]*len(script_list[i][-1])\n",
    "                data.iloc[i+1,0] =data.iloc[i+1,0] - ((data[\"end_time\"]-data[\"start_time\"])/len(data[\"script\"]))[i]*len(script_list[i][-1])\n",
    "                script_list[i+1][0]=script_list[i][-1] + script_list[i+1][0]\n",
    "                script_list[i][-1]=[]\n",
    "    final_script_list = []\n",
    "    for script in script_list :\n",
    "        sc_temp = \"\"\n",
    "        for sc in script :\n",
    "            if sc !=[] :\n",
    "                sc_temp = sc_temp+ str(sc)\n",
    "        final_script_list.append(sc_temp)\n",
    "    data['final_script']=final_script_list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1f227be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(data,kw) :\n",
    "    text = \"\"\n",
    "    for i in data[\"final_script\"] :\n",
    "        text += i + \" \"\n",
    "          \n",
    "    keyword_cnt = []\n",
    "    textrank = TextRank(text)\n",
    "    keywords=textrank.keywords(5)\n",
    "    if kw != \"\" :\n",
    "        keywords.append(kw)\n",
    "    for keyword in keywords :\n",
    "        keyword_cnt.append(text.count(keyword))\n",
    "    return keywords , keyword_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "476e0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary(data) :\n",
    "    text = \"\"\n",
    "    for i in data[\"final_script\"] :\n",
    "        text += i + \" \"\n",
    "        \n",
    "    mytokenizer: OktTokenizer = OktTokenizer()\n",
    "    lexrank: LexRank = LexRank(mytokenizer)\n",
    "\n",
    "    lexrank.summarize(text)\n",
    "\n",
    "    summaries: List[str] = lexrank.probe(0.05)\n",
    "    summary_c = \"\"\n",
    "    for summary in summaries:\n",
    "        summary_c += summary + \"\\n\"\n",
    "    return summary_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cd9c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9060f81c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug: * Running on http://localhost:9000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  ======작동======\n",
      "JSON확인 :  True\n",
      "feelwrite\n",
      "52\n",
      "공무원_회계학.mp4\n",
      "keyword : 회계\n",
      "MoviePy - Writing audio in C:/Users/SMHRD/git/HumanLearning/HumanLearning2/WebContent/resource/storage/공무원_회계학.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 76/76 [06:02<00:00,  4.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [17/Aug/2021 21:27:30] \"POST /test HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모두 완료\n",
      "                  ======작동======\n",
      "JSON확인 :  True\n",
      "feelwrite\n",
      "70\n",
      "공무원_행정법.mp4\n",
      "keyword : 행정\n",
      "MoviePy - Writing audio in C:/Users/SMHRD/git/HumanLearning/HumanLearning2/WebContent/resource/storage/공무원_행정법.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [07:15<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [23/Aug/2021 16:19:54] \"POST /test HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모두 완료\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, redirect, url_for,request\n",
    "# from werkzeug import secure_filename\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "#@app.route('/upload')\n",
    "#def renderHtml():\n",
    "#    return render_template(upload.html)\n",
    "\n",
    "#파일 업로드\n",
    "@app.route('/test',methods=['GET','POST']) #GET = 특정 url을 요청, POST = 값을 넘겨주며 url을 요청\n",
    "def uploadFile():\n",
    "    if request.method=='POST':\n",
    "        print('                  ======작동======')\n",
    "        print('JSON확인 : ',request.is_json)\n",
    "        contents=request.get_json(force=True,silent=True)\n",
    "        print(contents['user_id'])\n",
    "        print(contents['note_id'])\n",
    "        print(contents['filename'])\n",
    "        \n",
    "        kw = contents['keyword']\n",
    "        print(\"keyword :\",kw)\n",
    "        user_id = contents['user_id']\n",
    "        note_id = contents['note_id']\n",
    "        \n",
    "        path_dir = 'C:/Users/SMHRD/git/HumanLearning/HumanLearning2/WebContent/resource/storage/'\n",
    "        file_name = contents['filename']\n",
    "        #moviepy.editor 이용\n",
    "        #mp4 -> mp3 변환\n",
    "        video = VideoFileClip(os.path.join(path_dir+file_name))\n",
    "\n",
    "        video.audio.write_audiofile(os.path.join(path_dir+file_name[:-4]+\".mp3\"))\n",
    "        # mp3 -> wav 변환\n",
    "        AudioSegment.from_mp3(path_dir+file_name[:-4]+\".mp3\").export(path_dir+file_name[:-4]+\".wav\", format=\"wav\", bitrate=\"16k\")\n",
    "        print(0)\n",
    "        #재생시간 구하기\n",
    "        duration = cal_duration(path_dir+file_name[:-4]+\".wav\")\n",
    "        print(1)\n",
    "        # 스크립트 시간대 구하기\n",
    "        time_list2 = cut_time(path_dir+file_name[:-4]+\".wav\",duration)\n",
    "        print(2)\n",
    "        #stt를 통해 텍스트 뽑기\n",
    "        data = create_stt_script(path_dir+file_name[:-4]+\".wav\",time_list2,duration)\n",
    "        print(3)\n",
    "        # 텍스트 문장맺음 보정하기\n",
    "        data = modify_script(data)\n",
    "        print(4)\n",
    "        \n",
    "        # 키워드 뽑기\n",
    "\n",
    "        keywords,keyword_cnt=extract_keywords(data,kw)\n",
    "\n",
    "        # 요약문 뽑기 lexrank라이브러리 \n",
    "        summary_c=extract_summary(data)\n",
    "\n",
    "        print(5)\n",
    "        # DB전송\n",
    "        ## VIDEOS 테이블 insert\n",
    "        insert_videos((user_id,note_id,file_name,duration,summary_c))\n",
    "        ## VIDEO_ANALYSES 테이블 insert\n",
    "        video_id = select_videos_seq()[0][0]\n",
    "        for i in range(data.shape[0]) :    \n",
    "            insert_video_analyses((user_id,video_id,round(data.iloc[i,0],1),round(data.iloc[i,1],1),data.iloc[i,3]))\n",
    "        ## KEYWORDS 테이블 insert\n",
    "        for i in range(len(keywords)) : \n",
    "            insert_keywords((user_id,video_id,keywords[i],keyword_cnt[i]))\n",
    "        print(\"모두 완료\")\n",
    "    return '완료'\n",
    "\n",
    "#서버 실행 부분\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"localhost\",port=\"9000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4fe9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_videos_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59831ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
